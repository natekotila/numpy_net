# Neural Network implemented in Numpy

After taking the deeplearning.ai course, I wanted to implement my own neural network from scratch.

As an initial network to develop, my focus was on creating a logistic image detector.

## Activation Functions:

1. Sigmoid: Used for the output layer, for binary classification
2. Tanh: Hyperbolic tangent, which can be useful as a hidden layer activation
3. ReLU: Rectified Linear Unit, quick calculation to make hidden layers compute quickly

## Forward Propagation

