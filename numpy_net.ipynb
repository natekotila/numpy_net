{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Activation Functions\n",
    "During each step in the neural network, there are multiple options for activation functions:\n",
    "\n",
    "$\\sigma\\left(z\\right) = \\frac{1}{1 + e^{-z}}$<br>\n",
    "RELU$\\left(z\\right) = \\text{max}\\left(z, 0\\right)$<br>\n",
    "$\\tanh\\left(z\\right)$<br>\n",
    "$Z\\left(X\\right) = WX + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Parameters: Real valued matrix z\n",
    "    \n",
    "    Returns: sigmoid activation funcion of z for each value in the matrix\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(Z):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the sigmoid evaluated at z\n",
    "    Parameters:\n",
    "        Z: array_like\n",
    "    Returns:\n",
    "        sg: array_like\n",
    "           (1- sigmoid(Z))(sigmoid(Z))    \n",
    "    \"\"\"\n",
    "    s = sigmoid(Z)\n",
    "    sg = s * (1 - s)\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Calculates the hyperbolic tangent of Z\n",
    "    Parameters:\n",
    "        Z: array_like\n",
    "    Returns:\n",
    "        tan_h: array_like\n",
    "            Hyperbolic Tangent tanh(z) = (exp(z) + exp(-z)) / (exp(z) - exp(-z))\n",
    "    \n",
    "    \"\"\"\n",
    "    numerator = np.exp(Z) - np.exp(-Z)\n",
    "    denominator = np.exp(Z) + np.exp(-Z)\n",
    "    tan_h =  numerator / denominator\n",
    "    \n",
    "    return tan_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_grad(Z):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of tanh(z) for use in back propagation\n",
    "    Parameters:\n",
    "        z: array_like\n",
    "    Returns:\n",
    "        tg: array_like\n",
    "            Derivative of tanh(Z) = 1 - tanh^2(Z)\n",
    "    \"\"\"\n",
    "    t = tanh(Z)\n",
    "    tg = 1 - t ** 2\n",
    "    \n",
    "    return tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Unit of a real value Z\n",
    "    Parameter:\n",
    "        Z: array_like\n",
    "    Returns:\n",
    "        r: array_like\n",
    "            Elementwise Maximum (Z, 0)\n",
    "    \"\"\"\n",
    "    #Absolute value of Z is used so as to avoid getting -0. values\n",
    "    r = (Z > 0) * abs(Z)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_grad(Z):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the ReLu function\n",
    "    Parameters:\n",
    "        Z: array_like\n",
    "    Returns:\n",
    "        rg: array_like\n",
    "            1 if Z > 0, 0 otherwise\n",
    "    \"\"\"\n",
    "    rg = 1. * (Z > 0) + .01 * (Z < 0)\n",
    "    return rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_cost(Y_hat, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    arg = -9 * Y * np.log(Y_hat) - 1 * (1 - Y) * np.log(1 - Y_hat)\n",
    "    cost = 1 / m * np.sum(arg)\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.24979187e-01 1.00000000e+00 3.05902227e-07 7.31058579e-01\n",
      " 2.68941421e-01]\n",
      "[ 0.09966799  1.         -1.          0.76159416 -0.76159416]\n",
      "[ 0.1 24.   0.   1.   0. ]\n"
     ]
    }
   ],
   "source": [
    "# sigmoid test values\n",
    "z = np.array([0.1, 24, -15, 1, -1])\n",
    "print(sigmoid(z))\n",
    "print(tanh(z))\n",
    "print(relu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases. \n",
    "    Each set of weights will be in the form of a matrix of layer_dims[l] by layer_dims[l-1] dimensions\n",
    "        and will be randomly initialized to be near zero.\n",
    "    Biases will all be column vectors of dimension layer_dims[l] initialized to zero.\n",
    "    Parameters: \n",
    "        layer_dims: The number of nodes in each layer of the network, excluding the output layer\n",
    "    \n",
    "    Returns:\n",
    "        params: A dictionary containing the weights Wl and Biases bl for each layer l of the network\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    \n",
    "    for l in range(1,L):\n",
    "            params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * .1\n",
    "            params['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.02750639, -0.03398146],\n",
      "       [ 0.05493924,  0.04214238],\n",
      "       [ 0.08291598, -0.11357557],\n",
      "       [-0.05927144,  0.0555516 ]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.1831332 , -0.05982654,  0.04467447, -0.33754827],\n",
      "       [-0.07598296, -0.06319015, -0.0710485 ,  0.33300061]]), 'b2': array([[0.],\n",
      "       [0.]]), 'W3': array([[0.12869064, 0.04723863]]), 'b3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "# Test for parameter initialization.\n",
    "layer_dims = [2,4,2, 1]\n",
    "params = initialize_weights(layer_dims)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(A_prev, W, b, activation = 'relu'):\n",
    "    \"\"\"\n",
    "    Single Step of Forward Propagation for m samples\n",
    "    Parameters:\n",
    "        A_prev : array_like \n",
    "            Dimensions: (layer_dim, m)\n",
    "            Real valued input for linear transformation\n",
    "            Activation function output for previous step, or X if this is the first step\n",
    "        W : array_like\n",
    "            Dimensions:(A.shape[0], A_prev.shape[0])\n",
    "            Weight Matrix\n",
    "            \n",
    "        b : \n",
    "            Bias Vector\n",
    "            Dimension(A.shape[0], 1)\n",
    "        activation : String\n",
    "            Determines which function is performed after linear step for.\n",
    "            Choices are: 'relu' Regularized Linear Unit\n",
    "                         'sigmoid' Sigmoid Function\n",
    "                         'tanh' Hypebolic Tangent Function           \n",
    "    Returns:\n",
    "        A: array_like\n",
    "           Matrix for Activation layer\n",
    "        Z: array_like\n",
    "           Linear transformation of X with Weights and Bias\n",
    "           Stored for back propagation step\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = linear(A_prev, W, b)\n",
    "    \n",
    "    if (activation == 'relu'):\n",
    "        A = relu(Z)\n",
    "    elif (activation == 'sigmoid'):\n",
    "        A = sigmoid(Z)\n",
    "    elif (activation == 'tanh'):\n",
    "        A = tanh(Z)\n",
    " \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_L_layers(X, params, activations = None):\n",
    "    \"\"\"\n",
    "    Implementation of Forward Propagation through all layers of a Deep Network\n",
    "    \n",
    "    Parameters:\n",
    "        X : array-like\n",
    "            Input values for neural network\n",
    "      \n",
    "        params: array-like\n",
    "                Weights and biases calculated at this point.\n",
    "      \n",
    "      \n",
    "        activations : string-list\n",
    "                      Optional argument for using different activation functions throughout the network.\n",
    "                      Default is ReLU for hidden layers and Sigmoid for Output Layer\n",
    "                    \n",
    "    Returns:\n",
    "        y_hat : array-like\n",
    "                Predictions for the current loop through the network\n",
    "        \n",
    "        \n",
    "        caches : dictionary\n",
    "                 Computed Z, A, W, b values at each layer of the network. \n",
    "                 Stored for future use in Backward Propagation.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    A_prev = X\n",
    "    L = len(params) // 2\n",
    "    caches = {}\n",
    "    caches['A0'] = X\n",
    "    #Loop Through the Hidden Layers of the Network layers 1 -> L-1\n",
    "    for l in range(1, L):\n",
    "        A_prev, caches['Z' + str(l)] = forward_propagation(\n",
    "                                                            A_prev, params['W' + str(l)], \n",
    "                                                            params['b' + str(l)], \n",
    "                                                            activation = 'relu')\n",
    "        caches['A' + str(l)] = A_prev\n",
    "    \n",
    "    #Output Layer of Network\n",
    "    y_hat, caches['Z' + str(L)] = forward_propagation( \n",
    "                                                       A_prev, params['W' + str(L)], \n",
    "                                                       params['b' + str(L)], \n",
    "                                                       activation = 'sigmoid')\n",
    "    caches['A' + str(L)] = y_hat\n",
    "\n",
    "    return y_hat, caches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(4, 2)\n",
      "(1, 5)\n",
      "(1, 5)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'back_propagation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-26a3bec65d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#print(cache)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'back_propagation' is not defined"
     ]
    }
   ],
   "source": [
    "# #Test Cell to Check Forward Prop\n",
    "# np.random.seed(0)\n",
    "# X = np.random.randn(2,5)\n",
    "# print(X.shape)\n",
    "# W1 = params['W1']\n",
    "# print(W1.shape)\n",
    "# b1 = params['b1']\n",
    "# y_hat, cache = forward_propagation_L_layers(X, params)\n",
    "# #np.dot(W1, X)\n",
    "# print(y_hat.shape)\n",
    "# #grads = back_propagation()\n",
    "# Y = np.array([[1,0,1,0,1]])\n",
    "# print(Y.shape)\n",
    "# #print(cache)\n",
    "# grads = back_propagation(Y, cache, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grads['dW3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = np.array([[1],[1],[0]])\n",
    "# np.random.seed(0)\n",
    "# print(Y.shape)\n",
    "# Y_hat = np.random.rand(Y.shape[0], Y.shape[1])\n",
    "# logistic_cost(Y_hat, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(Y, cache, params):\n",
    "    grads = {}\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    y_hat = cache['A' + str(L)]\n",
    "    m = y_hat.shape[1]\n",
    "    \n",
    "    grads['dA' + str(L)] = -9 * np.divide(Y, y_hat) + np.divide(1 - Y, 1 - y_hat)\n",
    "    grads['dZ' + str(L)] = grads['dA' + str(L)] * sigmoid_grad(cache['Z' + str(L)])\n",
    "    assert grads['dZ' + str(L)].shape == cache['Z' + str(L)].shape\n",
    "    \n",
    "    grads['dW' + str(L)] = 1 / m * np.dot(grads['dZ' + str(L)], cache['A' + str(L-1)].T)\n",
    "    grads['db' + str(L)] = 1 / m * np.sum(grads['dZ' + str(L)], axis = 1, keepdims = True)\n",
    "    \n",
    "    for l in reversed(range(1,L)):\n",
    "        grads['dA' + str(l)] = np.dot(params['W' + str(l + 1)].T, grads['dZ' + str(l + 1)])\n",
    "        \n",
    "        grads['dZ' + str(l)] = np.multiply(grads['dA' + str(l)], relu_grad(cache['Z' + str(l)]))\n",
    "        assert grads['dZ' + str(l)].shape == cache['Z' + str(l)].shape\n",
    "        \n",
    "        grads['dW' + str(l)] = 1 / m * np.dot(grads['dZ' + str(l)], cache['A' + str(l - 1)].T)\n",
    "        assert grads['dW' + str(l)].shape == params['W' + str(l)].shape\n",
    "        \n",
    "        grads['db' + str(l)] = 1 / m * np.sum(grads['dZ' + str(l)], axis = 1, keepdims = True)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        params['W' + str(l + 1)] -= learning_rate * grads['dW' + str(l + 1)]\n",
    "        params['b' + str(l + 1)] -= learning_rate * grads['db' + str(l + 1)]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X,y = load_digits(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_5 = (y == 5) * 1.\n",
    "y_5 = y_5.reshape(1, 1797)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.reshape(64, 1797)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(X, Y, epochs=100, learning_rate=.01):\n",
    "    n_X = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    layer_dims = np.array([n_X, 6, 4, 2, n_y])\n",
    "    \n",
    "    parameters = initialize_weights(layer_dims)\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_hat, cache = forward_propagation_L_layers(X, parameters)\n",
    "        #print(y_hat.shape, Y.shape)\n",
    "        assert y_hat.shape == Y.shape\n",
    "        cost = logistic_cost(y_hat, Y)\n",
    "        costs.append(cost)\n",
    "        #print(cost)\n",
    "        grads = back_propagation(Y, cache, parameters)\n",
    "        parameters = update_params(parameters, grads, learning_rate)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_X = X_new.shape[0]\n",
    "n_y = y_5.shape[0]\n",
    "\n",
    "layer_dims = np.array([n_X, 8, 4, 8, n_y])\n",
    "\n",
    "learning_rate = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_weights(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, cache = forward_propagation_L_layers(X_new, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.append(logistic_cost(y_hat, y_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = back_propagation(y_5, cache, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01436643,  0.14541927,  0.07608131,  0.0121304 ,  0.04437591,\n",
       "          0.03333104,  0.14935692, -0.02053929,  0.03127279, -0.08541995,\n",
       "         -0.25531865,  0.06531124,  0.08642317, -0.07424086,  0.22694709,\n",
       "         -0.14544511,  0.0045391 , -0.01872998,  0.15324936,  0.14691592,\n",
       "          0.01549155,  0.03778925, -0.08878406, -0.19811761, -0.0348557 ,\n",
       "          0.01562651,  0.12300194,  0.12021873, -0.03872594, -0.03024893,\n",
       "         -0.10487602, -0.14202561, -0.17066239,  0.19506488, -0.05100484,\n",
       "         -0.04383304, -0.12530033,  0.07772716, -0.16138019, -0.0213079 ,\n",
       "         -0.08959321,  0.03866249, -0.05110189, -0.11806468, -0.00283628,\n",
       "          0.04281043,  0.0065932 ,  0.03021912, -0.06345831, -0.03629669,\n",
       "         -0.06726059, -0.03599665, -0.08132452, -0.17265769,  0.01769542,\n",
       "         -0.04019909, -0.16305015,  0.04626245, -0.09074607,  0.00515213,\n",
       "          0.07291718,  0.01286243,  0.11391766, -0.12350724],\n",
       "        [ 0.04028178, -0.06848711, -0.08693106, -0.05792906, -0.03109239,\n",
       "          0.00576172, -0.11659102,  0.0902188 ,  0.04663095, -0.15364414,\n",
       "          0.14896608,  0.18957734,  0.11795781, -0.01790417, -0.10713925,\n",
       "          0.10558784, -0.04026509,  0.12223284,  0.0209799 ,  0.09763326,\n",
       "          0.03569268,  0.07075037,  0.00100543,  0.17873127,  0.01272464,\n",
       "          0.04019895,  0.18844561, -0.13479758, -0.12698696,  0.09706017,\n",
       "         -0.11736358,  0.19449872, -0.04130422, -0.07476284,  0.19245524,\n",
       "          0.14803508,  0.18684065,  0.0907282 , -0.08616294,  0.19115172,\n",
       "         -0.0267303 ,  0.08024532,  0.09486667, -0.01551899,  0.06146998,\n",
       "          0.09231887,  0.03762689, -0.10981104,  0.02987004,  0.13264059,\n",
       "         -0.06931909, -0.01498818, -0.04344662,  0.18504594,  0.06720558,\n",
       "          0.04089893, -0.07692728,  0.05390839, -0.06727616,  0.00316998,\n",
       "         -0.06354991,  0.06774192,  0.0576074 , -0.02068397],\n",
       "        [ 0.03985825, -0.1091849 , -0.14902854,  0.04423439,  0.016756  ,\n",
       "          0.06371672,  0.23861406,  0.09453224, -0.09104277,  0.11190497,\n",
       "         -0.13151735, -0.04590696, -0.00674226,  0.17152628, -0.07425497,\n",
       "         -0.08258338, -0.00957289, -0.0661881 ,  0.11278861, -0.10774925,\n",
       "         -0.11467613, -0.04361809, -0.04959466,  0.1930669 ,  0.09524302,\n",
       "          0.00889607, -0.12246763,  0.08471803, -0.09993143, -0.15431584,\n",
       "          0.11908534,  0.03177397,  0.09235338,  0.03203579,  0.08581967,\n",
       "         -0.06486727, -0.10332576,  0.06834015, -0.08011662, -0.06889332,\n",
       "         -0.04529081,  0.00192066, -0.03530484, -0.13726738, -0.06428127,\n",
       "         -0.22222675,  0.06277191, -0.16016302, -0.11022538,  0.0053755 ,\n",
       "         -0.0738671 ,  0.15462028, -0.12921163,  0.02692307, -0.00368434,\n",
       "         -0.11674833,  0.05257112, -0.01699901,  0.07728376,  0.08262919,\n",
       "          0.21644913,  0.13384424, -0.03667316, -0.02386289],\n",
       "        [ 0.10996773,  0.06552686,  0.06401686, -0.16169495, -0.0024321 ,\n",
       "         -0.07379928,  0.02799234, -0.00981327,  0.09102098,  0.03172252,\n",
       "          0.07863624, -0.04664059, -0.094443  , -0.04100347, -0.00170193,\n",
       "          0.03791744,  0.22593382, -0.00422503, -0.09559195, -0.03459787,\n",
       "         -0.04635804,  0.04815043, -0.15407981,  0.00632832,  0.01565308,\n",
       "          0.02321851, -0.0597303 , -0.02379063, -0.14240601, -0.04932943,\n",
       "         -0.05428612,  0.04160771, -0.11561624,  0.0781203 ,  0.14945298,\n",
       "         -0.20699824,  0.0426276 ,  0.06769356, -0.06374309, -0.03972474,\n",
       "         -0.01328599, -0.02977807, -0.03089767, -0.1675997 ,  0.11523409,\n",
       "          0.10796406, -0.08133639, -0.14664169,  0.05210878, -0.05757772,\n",
       "          0.01419837, -0.03193131,  0.06915495,  0.06947686, -0.07255916,\n",
       "         -0.13833387, -0.15829242,  0.06103897, -0.1188838 , -0.05068086,\n",
       "         -0.05963034, -0.0052549 , -0.19362805,  0.01887971],\n",
       "        [ 0.05223731,  0.00872533, -0.03135764,  0.00963743,  0.03973342,\n",
       "         -0.27751823,  0.19548575,  0.03874637, -0.06542826, -0.03922525,\n",
       "          0.04912087, -0.01173414, -0.20325921,  0.20620939, -0.01115761,\n",
       "          0.10176053, -0.06938412,  0.15348839,  0.02834852,  0.06074323,\n",
       "         -0.10469059,  0.12089885,  0.06887282,  0.12990306, -0.06300783,\n",
       "         -0.04824253,  0.23011454, -0.10613018, -0.013762  ,  0.11343307,\n",
       "          0.00966067,  0.0580336 , -0.04014081,  0.03686964, -0.1309018 ,\n",
       "          0.16566444, -0.01202648, -0.06825945,  0.06653169, -0.04632116,\n",
       "         -0.13361141, -0.13480474,  0.06911539, -0.01607124, -0.01353278,\n",
       "          0.10755371, -0.11279379, -0.07327955, -0.03865645,  0.00929379,\n",
       "         -0.00448513, -0.028828  , -0.00632648, -0.01097831, -0.07206391,\n",
       "         -0.08154855,  0.02726022, -0.0891902 , -0.11598278, -0.03134691,\n",
       "         -0.01596031,  0.22543754, -0.07056151,  0.09406568],\n",
       "        [ 0.07470155, -0.11889213,  0.07732249, -0.11839686, -0.26591585,\n",
       "          0.0606211 , -0.17559031,  0.04509426, -0.06841142,  0.16595004,\n",
       "          0.10684447, -0.04534568, -0.06878244, -0.12141648, -0.04409825,\n",
       "         -0.02803577, -0.03648347,  0.0156657 ,  0.05784501,  0.0349478 ,\n",
       "         -0.07641372, -0.14378354,  0.13644578, -0.06894451, -0.06524266,\n",
       "         -0.05212377, -0.18430633, -0.04780629, -0.04796443,  0.06202805,\n",
       "          0.06983286,  0.00037711,  0.09316719,  0.03399114, -0.00157456,\n",
       "          0.01607972, -0.01906526, -0.03948982, -0.02677851, -0.11279939,\n",
       "          0.028033  , -0.09931237,  0.08415649, -0.02495516,  0.00495067,\n",
       "          0.04937388,  0.06432369, -0.15706147, -0.02070369,  0.08801212,\n",
       "         -0.1698107 ,  0.03871281, -0.2255547 , -0.10225847,  0.0038569 ,\n",
       "         -0.1656691 , -0.09856377, -0.14718481,  0.16480647,  0.01640866,\n",
       "          0.05673006, -0.02227598, -0.03535342, -0.16164501],\n",
       "        [-0.02920671, -0.07614037,  0.08573198,  0.11412383,  0.14663829,\n",
       "          0.08519946, -0.05983637, -0.11163541,  0.07664023,  0.03564028,\n",
       "         -0.17689447,  0.03554621,  0.08142534,  0.00585956, -0.01848091,\n",
       "         -0.08081018, -0.14467608,  0.0800363 , -0.03096509, -0.02334125,\n",
       "          0.17325205,  0.06841153,  0.03709765,  0.01415488,  0.15197783,\n",
       "          0.17196069,  0.09290075,  0.05823104, -0.20947498,  0.01232925,\n",
       "         -0.01298578,  0.00934541,  0.09427699, -0.27395254, -0.0569858 ,\n",
       "          0.02698896, -0.04671642, -0.14173595,  0.08691151,  0.02763474,\n",
       "         -0.09713245,  0.0314803 ,  0.08210795,  0.00053523,  0.08003885,\n",
       "          0.0077862 , -0.0395234 , -0.11598158, -0.00861205,  0.01942949,\n",
       "          0.08752789, -0.0115024 ,  0.04572357, -0.09650179, -0.07825147,\n",
       "         -0.01108568, -0.1054815 ,  0.08203338,  0.04625412,  0.02791336,\n",
       "          0.03388681,  0.20205707, -0.04686801, -0.22018778],\n",
       "        [ 0.01991402, -0.00508716, -0.05178616, -0.09789355, -0.04395205,\n",
       "          0.01809814, -0.0502935 ,  0.24120358, -0.09606637, -0.07933531,\n",
       "         -0.22889176,  0.02513367, -0.20167983, -0.05396407, -0.02758551,\n",
       "         -0.07101051,  0.17386531,  0.0994121 ,  0.13187311, -0.08825198,\n",
       "          0.11282288,  0.0495744 ,  0.07712411,  0.10290344, -0.09089341,\n",
       "         -0.04245911,  0.086225  , -0.26557089,  0.15129752,  0.05528639,\n",
       "         -0.00458568,  0.02200793, -0.10301077, -0.03501828,  0.10998648,\n",
       "          0.12978752,  0.26957676, -0.00742025, -0.06587289, -0.05146326,\n",
       "         -0.10182131, -0.00781515,  0.03823831, -0.00343709,  0.10959806,\n",
       "         -0.02344611, -0.03476398, -0.05816178, -0.16327759, -0.15680163,\n",
       "         -0.11794523,  0.13012471,  0.0894902 ,  0.13746767, -0.1332358 ,\n",
       "         -0.19690409, -0.0660188 ,  0.01755434,  0.04982734,  0.10478455,\n",
       "          0.02839129,  0.17424024, -0.02227524, -0.09134586]]),\n",
       " 'W2': array([[-0.16812125, -0.08881889,  0.02422376, -0.08886995,  0.09373748,\n",
       "          0.14123448, -0.2369568 ,  0.08645024],\n",
       "        [-0.22390211,  0.04722128,  0.12329006,  0.0066643 , -0.12733323,\n",
       "         -0.0585217 , -0.02504007, -0.01703394],\n",
       "        [-0.02030394, -0.01102703,  0.02124329, -0.12085654, -0.02420572,\n",
       "          0.15181457, -0.03848168, -0.04437953],\n",
       "        [ 0.10812268, -0.25583331,  0.11837017, -0.06319073,  0.01644764,\n",
       "          0.0096361 ,  0.09449597, -0.02673308]]),\n",
       " 'W3': array([[-0.06780047,  0.12994237, -0.23641595,  0.00212717],\n",
       "        [-0.1347924 , -0.0761558 ,  0.20112078, -0.0044656 ],\n",
       "        [ 0.01951968, -0.17815864, -0.07290428,  0.01978762],\n",
       "        [ 0.03560964,  0.06391298,  0.00091473,  0.05407706],\n",
       "        [ 0.04535834, -0.18297828,  0.00369664,  0.07658266],\n",
       "        [ 0.05897305, -0.03638378, -0.08056258, -0.11182919],\n",
       "        [-0.01310375,  0.11346197, -0.19517949, -0.06597961],\n",
       "        [-0.11397789,  0.07925018, -0.05542483, -0.04702324]]),\n",
       " 'W4': array([[-0.02262508,  0.04452101, -0.03930876, -0.30531867,  0.05404774,\n",
       "          0.04388503, -0.02270821, -0.10892454]]),\n",
       " 'b1': array([[-5.17572796e-06],\n",
       "        [ 1.09384768e-05],\n",
       "        [ 3.55261115e-05],\n",
       "        [ 3.35285907e-07],\n",
       "        [-3.73723846e-05],\n",
       "        [-1.26375045e-06],\n",
       "        [-3.82520120e-06],\n",
       "        [-5.37868462e-06]]),\n",
       " 'b2': array([[ 9.92760673e-06],\n",
       "        [ 6.60673293e-04],\n",
       "        [-1.87626219e-05],\n",
       "        [ 9.12811688e-05]]),\n",
       " 'b3': array([[ 5.54003937e-04],\n",
       "        [-1.15766970e-05],\n",
       "        [ 2.52851298e-04],\n",
       "        [ 8.29987838e-03],\n",
       "        [-3.87941203e-04],\n",
       "        [-3.97748343e-05],\n",
       "        [ 4.25127220e-04],\n",
       "        [ 2.08401039e-03]]),\n",
       " 'b4': array([[-0.03968457]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_params(parameters, grads, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = run_nn(X_new, y_5, epochs=12000, learning_rate=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24796c0e908>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x247969eecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.squeeze(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y_5.shape[1]\n",
    "y_hat, _ = forward_propagation_L_layers(X_new, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((y_5 == 1) * (y_hat > .5))\n",
    "tn = np.sum((y_5 == 0) * (y_hat < .5))\n",
    "pos = np.sum(y_5 == 1)\n",
    "neg = np.sum(y_5 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_accuracy = tp / pos\n",
    "neg_accuracy = tn / neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9380804953560371"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9443516972732332"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
